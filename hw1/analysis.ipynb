{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "hnI_rU-WYmuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restaurants = '/content/drive/MyDrive/διαλογθε_συστεμσ/span_extraction/restaurant8k/train_0.json'\n",
        "buses = '/content/drive/MyDrive/διαλογθε_συστεμσ/span_extraction/dstc8/Buses_1/train_0.json'\n",
        "events = '/content/drive/MyDrive/διαλογθε_συστεμσ/span_extraction/dstc8/Events_1/train_0.json'\n",
        "homes = '/content/drive/MyDrive/διαλογθε_συστεμσ/span_extraction/dstc8/Homes_1/train_0.json'\n",
        "car_rentals = '/content/drive/MyDrive/διαλογθε_συστεμσ/span_extraction/dstc8/RentalCars_1/train_0.json'\n",
        "directories = [restaurants, buses, events, homes, car_rentals]\n",
        "\n",
        "dataframes = {}\n",
        "\n",
        "def calculate_metrics(df):\n",
        "    # Total turn count\n",
        "    total_turn_count = len(df)\n",
        "\n",
        "    # Tokenize the text and calculate turn length\n",
        "    df['turn_length'] = df['userInput'].apply(lambda x: len(x['text'].split()))\n",
        "\n",
        "    # Tokenize the text and calculate sentence count\n",
        "    df['sent_count'] = df['userInput'].apply(lambda x: len(nltk.sent_tokenize(x['text'])))\n",
        "\n",
        "    # Total sentence count\n",
        "    total_sent_count = df['sent_count'].sum()\n",
        "\n",
        "    # Mean sentence count\n",
        "    mean_sent_count = round(df['sent_count'].mean(), 3)\n",
        "\n",
        "    # Standard deviation of sentence count\n",
        "    std_sent_count = round(df['sent_count'].std(), 3)\n",
        "\n",
        "    # Tokenize the text and calculate word count\n",
        "    df['word_count'] = df['userInput'].apply(lambda x: len(nltk.word_tokenize(x['text'])))\n",
        "\n",
        "    # Total word count\n",
        "    total_word_count = df['word_count'].sum()\n",
        "\n",
        "    # Calculate mean word length as average number of words (tokens) per sentence\n",
        "    mean_word_length = round(total_word_count / total_sent_count, 3)\n",
        "\n",
        "    # Calculate standard deviation of word length\n",
        "    word_lengths = df['userInput'].apply(lambda x: [len(word) for word in nltk.word_tokenize(x['text'])])\n",
        "    all_word_lengths = [length for sublist in word_lengths for length in sublist]\n",
        "    std_word_length = round(np.std(all_word_lengths), 3)\n",
        "\n",
        "    # Calculate vocabulary size (unique words)\n",
        "    vocab_set = set()\n",
        "    for text in df['userInput']:\n",
        "        vocab_set.update(set(nltk.word_tokenize(text['text'])))\n",
        "    vocab_size = len(vocab_set)\n",
        "\n",
        "    # Calculate vocabulary size excluding stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    vocab_size_no_stopwords = len(vocab_set - stop_words)\n",
        "\n",
        "    return {\n",
        "        'total_turn_count': total_turn_count,\n",
        "        'total_sent_count': total_sent_count,\n",
        "        'mean_sent_count': mean_sent_count,\n",
        "        'std_sent_count': std_sent_count,\n",
        "        'total_word_count': total_word_count,\n",
        "        'mean_word_length': mean_word_length,\n",
        "        'std_word_length': std_word_length,\n",
        "        'vocab_size': vocab_size,\n",
        "        'vocab_size_no_stopwords': vocab_size_no_stopwords\n",
        "    }\n",
        "\n",
        "for directory in directories:\n",
        "    if os.path.exists(directory):\n",
        "        df = pd.read_json(directory)\n",
        "\n",
        "        category = os.path.basename(os.path.dirname(directory))\n",
        "\n",
        "        dataframes[category] = df\n",
        "    else:\n",
        "        print(f\"{directory} does not exist.\")\n",
        "\n",
        "def extract_metrics(dataframes):\n",
        "    metrics = {}\n",
        "    for category, df in dataframes.items():\n",
        "        metrics[category] = calculate_metrics(df)\n",
        "    return metrics\n",
        "\n",
        "metrics = extract_metrics(dataframes)"
      ],
      "metadata": {
        "id": "Y5VrGUWYUa0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npUnYavrCF8l",
        "outputId": "70c9ebb8-a708-4e3f-e89c-234729e9bd2e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'restaurant8k': {'total_turn_count': 8198,\n",
              "  'total_sent_count': 8673,\n",
              "  'mean_sent_count': 1.058,\n",
              "  'std_sent_count': 0.254,\n",
              "  'total_word_count': 68637,\n",
              "  'mean_word_length': 7.914,\n",
              "  'std_word_length': 2.354,\n",
              "  'vocab_size': 4484,\n",
              "  'vocab_size_no_stopwords': 4373},\n",
              " 'Buses_1': {'total_turn_count': 1133,\n",
              "  'total_sent_count': 1434,\n",
              "  'mean_sent_count': 1.266,\n",
              "  'std_sent_count': 0.495,\n",
              "  'total_word_count': 11377,\n",
              "  'mean_word_length': 7.934,\n",
              "  'std_word_length': 2.074,\n",
              "  'vocab_size': 513,\n",
              "  'vocab_size_no_stopwords': 443},\n",
              " 'Events_1': {'total_turn_count': 1498,\n",
              "  'total_sent_count': 1906,\n",
              "  'mean_sent_count': 1.272,\n",
              "  'std_sent_count': 0.502,\n",
              "  'total_word_count': 14562,\n",
              "  'mean_word_length': 7.64,\n",
              "  'std_word_length': 2.174,\n",
              "  'vocab_size': 786,\n",
              "  'vocab_size_no_stopwords': 706},\n",
              " 'Homes_1': {'total_turn_count': 2064,\n",
              "  'total_sent_count': 2636,\n",
              "  'mean_sent_count': 1.277,\n",
              "  'std_sent_count': 0.509,\n",
              "  'total_word_count': 19733,\n",
              "  'mean_word_length': 7.486,\n",
              "  'std_word_length': 2.324,\n",
              "  'vocab_size': 752,\n",
              "  'vocab_size_no_stopwords': 667},\n",
              " 'RentalCars_1': {'total_turn_count': 874,\n",
              "  'total_sent_count': 1095,\n",
              "  'mean_sent_count': 1.253,\n",
              "  'std_sent_count': 0.473,\n",
              "  'total_word_count': 8873,\n",
              "  'mean_word_length': 8.103,\n",
              "  'std_word_length': 1.98,\n",
              "  'vocab_size': 585,\n",
              "  'vocab_size_no_stopwords': 516}}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = []\n",
        "for directory in directories:\n",
        "    if os.path.exists(directory):\n",
        "        df = pd.read_json(directory)\n",
        "        dfs.append(df)\n",
        "\n",
        "combined_df = pd.concat(dfs, ignore_index=True)\n",
        "combined_metrics = calculate_metrics(combined_df)"
      ],
      "metadata": {
        "id": "TCzzPZmTQo2D"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOIbC7v3XpzF",
        "outputId": "b69be51a-751f-491e-f46d-7a7b07878608"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'total_turn_count': 13767,\n",
              " 'total_sent_count': 15744,\n",
              " 'mean_sent_count': 1.144,\n",
              " 'std_sent_count': 0.387,\n",
              " 'total_word_count': 123182,\n",
              " 'mean_word_length': 7.824,\n",
              " 'std_word_length': 2.281,\n",
              " 'vocab_size': 5281,\n",
              " 'vocab_size_no_stopwords': 5161}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tbfgGiAeY2PY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}